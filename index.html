<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Gautham Narayan Narasimhan</title>
  
  <meta name="author" content="Gautham Narayan Narasimhan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Gautham Narayan Narasimhan</name>
              </p>
              <p>I am a currently  Research Assistant at the Robotics Institute (RI) at Carnegie Mellon University. Previously I completed my masters thesis with
                <a href="https://davheld.github.io/">Prof. David Held</a>.
              </p>
              <p>
                My research goal is to learn efficient visual representations that enable robots to learn general purpose object manipulation skills.
                Currently I'm working on model based reinforcement learning for robotic pouring tasks.
              </p>

              <p style="text-align:center">
                <a href="gauthamnarayan@hotmail.com">Contact</a> &nbsp/&nbsp
                <a href="https://github.com/gauthamnarayan">GitHub</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=4m-uVKUAAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/gautham_dp_cropped.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/2020_ROLL.jpg" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>ROLL: Visual Self-Supervised Reinforcement Learning with Object Reasoning</papertitle><br>
              Yufei Wang, <strong>Gautham Narayan</strong>, Xingyu Lin, Brian Okorn, David Held<br>
              <em>Conference on Robot Learning, CoRL 2020</em><br>
              <p>Unknown object segmentation to learn a visual representation that can reason about occlusions. Our method achieves 
              state of the art on object manipulation benchmarking tasks.</p>
              <a href="https://arxiv.org/pdf/2011.06777.pdf">[Paper]</a>
              <a href="https://sites.google.com/andrew.cmu.edu/roll/home">[Website]</a><br>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/website">Source</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
